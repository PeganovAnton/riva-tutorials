{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_asr_asr-python-advanced-finetune-am-citrinet-tao-finetuning/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to fine-tune a Riva ASR Acoustic Model (Citrinet) with TAO Toolkit\n",
    "This tutorial walks you through how to fine-tune a Riva ASR acoustic model (Citrinet) with TAO Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial, we are going to discuss the Citrinet model, which is an end-to-end ASR model that takes in audio and produces text.\n",
    "\n",
    "Citrinet is a descendent of QuartzNet that features the squeeze-and-excitation (SE) block and sub-word tokenization and has a better accuracy/performance than QuartzNet.\n",
    "\n",
    "![CitriNet with CTC](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/_images/citrinet_vertical.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ASR using TAO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TAO launcher uses Docker containers under the hood, and **for our data and results directory to be visible to Docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the environment variables and the amount of shared memory available to the TAO launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The following code creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results, and cache. You should configure it for your specific use case so these directories are correctly visible to the Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HOST_DATA_DIR=/home/nvidia/Documents/launchpad/asr-finetune/data\n",
      "env: HOST_SPECS_DIR=/home/nvidia/Documents/launchpad/asr-finetune/specs\n",
      "env: HOST_RESULTS_DIR=/home/nvidia/Documents/launchpad/asr-finetune/results\n"
     ]
    }
   ],
   "source": [
    "# d\\Defining paths on the local host machine\n",
    "%env HOST_DATA_DIR=/home/nvidia/Documents/launchpad/asr-finetune/data\n",
    "%env HOST_SPECS_DIR=/home/nvidia/Documents/launchpad/asr-finetune/specs\n",
    "%env HOST_RESULTS_DIR=/home/nvidia/Documents/launchpad/asr-finetune/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $HOST_DATA_DIR\n",
    "! mkdir -p $HOST_SPECS_DIR\n",
    "! mkdir -p $HOST_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "tlt_configs = {\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_DATA_DIR\"],\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_SPECS_DIR\"],\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_RESULTS_DIR\"],\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.path.expanduser(\"~/.cache\"),\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"16G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         }\n",
    "   }\n",
    "}\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(tlt_configs, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Mounts\": [\n",
      "        {\n",
      "            \"source\": \"/home/nvidia/Documents/launchpad/asr-finetune/data\",\n",
      "            \"destination\": \"/data\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"/home/nvidia/Documents/launchpad/asr-finetune/specs\",\n",
      "            \"destination\": \"/specs\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"/home/nvidia/Documents/launchpad/asr-finetune/results\",\n",
      "            \"destination\": \"/results\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"/home/nvidia/.cache\",\n",
      "            \"destination\": \"/root/.cache\"\n",
      "        }\n",
      "    ],\n",
      "    \"DockerOptions\": {\n",
      "        \"shm_size\": \"16G\",\n",
      "        \"ulimits\": {\n",
      "            \"memlock\": -1,\n",
      "            \"stack\": 67108864\n",
      "        }\n",
      "    }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the Docker image versions and the tasks that it performs. You can also check by issuing `tao --help` or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration of the TAO Toolkit Instance\n",
      "\n",
      "dockers: \t\t\n",
      "\tnvidia/tao/tao-toolkit-tf: \t\t\t\n",
      "\t\tv3.22.05-tf1.15.5-py3: \t\t\t\t\n",
      "\t\t\tdocker_registry: nvcr.io\n",
      "\t\t\ttasks: \n",
      "\t\t\t\t1. augment\n",
      "\t\t\t\t2. bpnet\n",
      "\t\t\t\t3. classification\n",
      "\t\t\t\t4. dssd\n",
      "\t\t\t\t5. faster_rcnn\n",
      "\t\t\t\t6. emotionnet\n",
      "\t\t\t\t7. efficientdet\n",
      "\t\t\t\t8. fpenet\n",
      "\t\t\t\t9. gazenet\n",
      "\t\t\t\t10. gesturenet\n",
      "\t\t\t\t11. heartratenet\n",
      "\t\t\t\t12. lprnet\n",
      "\t\t\t\t13. mask_rcnn\n",
      "\t\t\t\t14. multitask_classification\n",
      "\t\t\t\t15. retinanet\n",
      "\t\t\t\t16. ssd\n",
      "\t\t\t\t17. unet\n",
      "\t\t\t\t18. yolo_v3\n",
      "\t\t\t\t19. yolo_v4\n",
      "\t\t\t\t20. yolo_v4_tiny\n",
      "\t\t\t\t21. converter\n",
      "\t\tv3.22.05-tf1.15.4-py3: \t\t\t\t\n",
      "\t\t\tdocker_registry: nvcr.io\n",
      "\t\t\ttasks: \n",
      "\t\t\t\t1. detectnet_v2\n",
      "\tnvidia/tao/tao-toolkit-pyt: \t\t\t\n",
      "\t\tv3.22.05-py3: \t\t\t\t\n",
      "\t\t\tdocker_registry: nvcr.io\n",
      "\t\t\ttasks: \n",
      "\t\t\t\t1. speech_to_text\n",
      "\t\t\t\t2. speech_to_text_citrinet\n",
      "\t\t\t\t3. speech_to_text_conformer\n",
      "\t\t\t\t4. action_recognition\n",
      "\t\t\t\t5. pointpillars\n",
      "\t\t\t\t6. pose_classification\n",
      "\t\t\t\t7. spectro_gen\n",
      "\t\t\t\t8. vocoder\n",
      "\t\tv3.21.11-py3: \t\t\t\t\n",
      "\t\t\tdocker_registry: nvcr.io\n",
      "\t\t\ttasks: \n",
      "\t\t\t\t1. text_classification\n",
      "\t\t\t\t2. question_answering\n",
      "\t\t\t\t3. token_classification\n",
      "\t\t\t\t4. intent_slot_classification\n",
      "\t\t\t\t5. punctuation_and_capitalization\n",
      "\tnvidia/tao/tao-toolkit-lm: \t\t\t\n",
      "\t\tv3.22.05-py3: \t\t\t\t\n",
      "\t\t\tdocker_registry: nvcr.io\n",
      "\t\t\ttasks: \n",
      "\t\t\t\t1. n_gram\n",
      "format_version: 2.0\n",
      "toolkit_version: 3.22.05\n",
      "published_date: 05/25/2022\n"
     ]
    }
   ],
   "source": [
    "! tao info --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Relevant Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TAO Docker.\n",
    "\n",
    "# The data is saved here:\n",
    "DATA_DIR = \"/data\"\n",
    "SPECS_DIR = \"/specs\"\n",
    "RESULTS_DIR = \"/results\"\n",
    "\n",
    "# Set the encryption key and use the same key for all commands.\n",
    "KEY = 'tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command structure for the TAO interface can be broken down as follows: `tao <task name> <subcommand>` <br> \n",
    "\n",
    "Let's see this in further detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Downloading Specs\n",
    "TAO's conversational AI toolkit works off of spec files which make it easy to edit hyperparameters on the fly. We can proceed to downloading the spec files. You may choose to modify/rewrite these specs or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command.<br>\n",
    "\n",
    "The `-o` argument indicates the folder where the default specification files will be downloaded. The `-r` argument instructs the script on where to save the logs. **Ensure the `-o` points to an empty folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-14 09:34:55,037 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-06-14 09:34:55,151 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.22.05-py3\n",
      "2022-06-14 09:34:55,399 [INFO] tlt.components.docker_handler.docker_handler: The required docker doesn't exist locally/the manifest has changed. Pulling a new docker.\n",
      "2022-06-14 09:34:55,400 [INFO] tlt.components.docker_handler.docker_handler: Pulling the required container. This may take several minutes if you're doing this for the first time. Please wait here.\n",
      "...\n",
      "Pulling from repository: nvcr.io/nvidia/tao/tao-toolkit-pyt\n",
      "2022-06-14 09:40:27,727 [INFO] tlt.components.docker_handler.docker_handler: Container pull complete.\n",
      "2022-06-14 09:40:27,727 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/home/nvidia/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "I0614 16:40:32.288787 140592244635456 font_manager.py:1443] generated new fontManager\n",
      "[NeMo I 2022-06-14 16:40:35 tlt_logging:20] Experiment configuration:\n",
      "    exp_manager:\n",
      "      task_name: download_specs\n",
      "      explicit_log_dir: /results/speech_to_text_citrinet\n",
      "    source_data_dir: /opt/conda/lib/python3.8/site-packages/conv_ai/asr/speech_to_text_ctc/experiment_specs\n",
      "    target_data_dir: /specs/speech_to_text_citrinet\n",
      "    workflow: conv_ai\n",
      "    \n",
      "Downloading default specs for conv_ai\n",
      "[NeMo I 2022-06-14 16:40:35 download_specs:82] Default specification files for conv_ai downloaded to '/specs/speech_to_text_citrinet'\n",
      "Default specification files for conv_ai downloaded.List of files: ['infer_onnx_citrinet.yaml', 'train_conformer_bpe_large.yaml', 'evaluate.yaml', 'train_citrinet_256.yaml', 'infer.yaml', 'dataset_convert_an4.yaml', 'create_tokenizer.yaml', 'train_citrinet_bpe.yaml', 'dataset_convert_ru.yaml', 'infer_onnx_conformer.yaml', 'dataset_convert_en.yaml', 'export.yaml', 'train_conformer_bpe_medium.yaml', 'train_conformer_bpe_small.yaml', 'finetune.yaml']\n",
      "[NeMo I 2022-06-14 16:40:35 download_specs:89] Experiment logs saved to '/results/speech_to_text_citrinet'\n",
      "\u001b[0m2022-06-14 09:40:36,411 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# delete the specs directory if it is already there to avoid errors\n",
    "! tao speech_to_text_citrinet download_specs \\\n",
    "    -r $RESULTS_DIR/speech_to_text_citrinet \\\n",
    "    -o $SPECS_DIR/speech_to_text_citrinet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will use the popular AN4 dataset. Let's download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://dldata-public.s3.us-east-2.amazonaws.com/an4_sphere.tar.gz  # for the original source, please visit http://www.speech.cs.cmu.edu/databases/an4/an4_sphere.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading, untar the dataset and move it to the correct directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -xvf an4_sphere.tar.gz \n",
    "! mv an4 $HOST_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step converts the `.mp3` files into `.wav` files and splits the data into training and testing sets. It also generates a \"meta-data\" file to be consumed by the data-loader for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tao speech_to_text_citrinet dataset_convert \\\n",
    "    -e $SPECS_DIR/speech_to_text_citrinet/dataset_convert_an4.yaml \\\n",
    "    -r $RESULTS_DIR/citrinet/dataset_convert \\\n",
    "    source_data_dir=$DATA_DIR/an4 \\\n",
    "    target_data_dir=$DATA_DIR/an4_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's listen to a sample audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change path of the file here\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "path = os.environ[\"HOST_DATA_DIR\"] + '/an4_converted/wavs/an268-mbmg-b.wav'\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do the actual training, we need to pre-process the text. This step is called subword tokenization that creates a subword vocabulary for the text. In Citrinet, the subword can be one or multiple characters. We can use the `create_tokenizer` command to create the tokenizer that generates the subword vocabulary for us for use in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet create_tokenizer \\\n",
    "-e $SPECS_DIR/speech_to_text_citrinet/create_tokenizer.yaml \\\n",
    "-r $RESULTS_DIR/citrinet/create_tokenizer \\\n",
    "manifests=$DATA_DIR/an4_converted/train_manifest.json \\\n",
    "output_root=$DATA_DIR/an4 \\\n",
    "vocab_size=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TAO interface enables you to configure the training parameters from the command-line interface. <br>\n",
    "\n",
    "The process of opening the training script, finding the parameters of interest (which might be spread across multiple files), and making the changes needed, is being replaced by a simple command-line interface.\n",
    "\n",
    "For example, if the number of epochs are needed to be modified along with a change in the learning rate, you can add `trainer.max_epochs=10` and `optim.lr=0.02` and train the model. Sample commands are given below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>A list of some of the customizable parameters along with their default values is as follows:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer:<br>\n",
    "<ul>  \n",
    "  <li>gpus: 1 </li>\n",
    "  <li>num_nodes: 1 </li>\n",
    "  <li>max_epochs: 5 </li>\n",
    "  <li>max_steps: null </li>\n",
    "  <li>checkpoint_callback: false </li>\n",
    "</ul>\n",
    "\n",
    "training_ds:\n",
    "<ul>  \n",
    "  <li>sample_rate: 16000 </li>\n",
    "  <li>batch_size: 32 </li>\n",
    "  <li>trim_silence: true </li>\n",
    "  <li>max_duration: 16.7 </li>\n",
    "  <li>shuffle: true </li>\n",
    "  <li>is_tarred: false </li>\n",
    "  <li>tarred_audio_filepaths: null </li>\n",
    "</ul>  \n",
    "\n",
    "validation_ds:\n",
    "<ul>  \n",
    "  <li>sample_rate: 16000 </li>\n",
    "  <li>batch_size: 32 </li>\n",
    "  <li>shuffle: false </li>\n",
    "</ul>  \n",
    "optim:\n",
    "<ul>\n",
    "  <li>name: adam </li>\n",
    "  <li>lr: 0.1 </li>\n",
    "  <li>betas: [0.9, 0.999] </li>\n",
    "  <li>weight_decay: 0.0001 </li>\n",
    "</ul>\n",
    "\n",
    "The following steps may take a considerable amount of time depending on the GPU being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training an ASR Citrinet model in TAO, we use the `tao speech_to_text_citrinet train` command with the following arguments:\n",
    "<ul>\n",
    "    <li>`-e`: Path to the spec file </li>\n",
    "    <li>`-g`: Number of GPUs to use </li>\n",
    "    <li>`-r`: Path to the results folder </li>\n",
    "    <li>`-m`: Path to the model </li>\n",
    "    <li>`-k`: User specified encryption key to use while saving/loading the model </li>\n",
    "    <li>Any overrides to the spec file. For example, `trainer.max_epochs`. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Citrinet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet train \\\n",
    "     -e $SPECS_DIR/speech_to_text_citrinet/train_citrinet_bpe.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -r $RESULTS_DIR/citrinet/train \\\n",
    "     training_ds.manifest_filepath=$DATA_DIR/an4_converted/train_manifest.json \\\n",
    "     validation_ds.manifest_filepath=$DATA_DIR/an4_converted/test_manifest.json \\\n",
    "     trainer.max_epochs=1 \\\n",
    "     training_ds.num_workers=4 \\\n",
    "     validation_ds.num_workers=4 \\\n",
    "     model.tokenizer.dir=$DATA_DIR/an4/tokenizer_spe_unigram_v32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model trained, we need to check how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet evaluate \\\n",
    "     -e $SPECS_DIR/speech_to_text_citrinet/evaluate.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/citrinet/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/citrinet/evaluate \\\n",
    "     test_ds.manifest_filepath=$DATA_DIR/an4_converted/test_manifest.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is trained, evaluated, and there is a need for fine-tuning, the following command can be used to fine-tune the ASR model. This step can also be used for transfer learning by making changes in the `train.json` and `dev.json` files to add new data.\n",
    "\n",
    "The list for customizations is the same as the training parameters with the exception for parameters which affect the model architecture. Also, instead of `training_ds` we have `finetuning_ds`.\n",
    "\n",
    "Note: If you want to proceed with a trained dataset for better inference results, you can find a `.nemo` model [here](\n",
    "https://ngc.nvidia.com/catalog/collections/nvidia:nemotrainingframework).\n",
    "\n",
    "Simply re-name the `.nemo` file to `.tlt` and pass it through the fine-tune pipeline.\n",
    "\n",
    "Note: The fine-tune spec files contain specifics to fine-tune the English model we just trained to Russian. If you want to proceed with English, ensure the changes are in the spec file `finetune.yaml` which you can find in the `SPEC_DIR` folder you mapped. Ensure to delete older fine-tuning checkpoints if you choose to change the language after fine-tuning it as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet finetune \\\n",
    "     -e $SPECS_DIR/speech_to_text_citrinet/finetune.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/citrinet/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/citrinet/finetune \\\n",
    "     finetuning_ds.manifest_filepath=$DATA_DIR/an4_converted/train_manifest.json \\\n",
    "     validation_ds.manifest_filepath=$DATA_DIR/an4_converted/test_manifest.json \\\n",
    "     trainer.max_epochs=1 \\\n",
    "     finetuning_ds.num_workers=20 \\\n",
    "     validation_ds.num_workers=20 \\\n",
    "     trainer.gpus=1 \\\n",
    "     tokenizer.dir=$DATA_DIR/an4/tokenizer_spe_unigram_v32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR model export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TAO, you can also export your model in a format that can deployed using NVIDIA Riva; a highly performant application framework for multi-modal conversational AI services using GPUs. The same command for exporting to ONNX can be used here. The only small variation is the configuration for `export_format` in the spec file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to Riva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet export \\\n",
    "     -e $SPECS_DIR/speech_to_text_citrinet/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/citrinet/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/citrinet/riva \\\n",
    "     export_format=RIVA \\\n",
    "     export_to=asr-model.riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to ONNX (Note: Export to ONNX is not needed for Riva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet export \\\n",
    "     -e $SPECS_DIR/speech_to_text_citrinet/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/citrinet/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/citrinet/export \\\n",
    "     export_format=ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR Inference using TLT checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ASR Inference with TAO Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to run inference on the tlt checkpoint with TAO Toolkit. \n",
    " For real-time inference and best latency, we need to deploy this model on Riva - Refer to [How to deploy custom Acoustic Model (Citrinet) trained with TAO Toolkit on Riva](https://github.com/nvidia-riva/tutorials/blob/dev/22.04-citrinet/asr-python-advanced-finetune-am-citrinet-tao-deployment.ipynb) tutorial. \n",
    " You might have to work with the infer.yaml file to select the files you want for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet infer \\\n",
    "     -e $SPECS_DIR/speech_to_text_citrinet/infer.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/citrinet/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/citrinet/infer \\\n",
    "     file_paths=[$DATA_DIR/an4_converted/wavs/an268-mbmg-b.wav]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ASR Inference using ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAO provides the capability to use the exported `.eonnx` model for inference. The command `tao speech_to_text infer_onnx` is very similar to the inference command for `.tlt` models. Again, the inputs in the spec file used is just for demo purposes, you may choose to try out your custom input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao speech_to_text_citrinet infer_onnx \\\n",
    "     -e $SPECS_DIR/speech_to_text_citrinet/infer_onnx_citrinet.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/citrinet/export/exported-model.eonnx \\\n",
    "     -r $RESULTS_DIR/infer_onnx \\\n",
    "     file_paths=[$DATA_DIR/an4_converted/wavs/an268-mbmg-b.wav]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained and fine-tuned Citrinet accoustic model, we can now deploy this custom model to NVIDIA Riva"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('riva': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d07f1aa6807adb4e3490764d3413abc2e022e4483e8e90f694051be99589cf55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
