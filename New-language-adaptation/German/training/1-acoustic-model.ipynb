{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Acoustic Model with subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqBQwLAsme9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: wget in /opt/conda/lib/python3.8/site-packages (3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:4.2.4-1ubuntu0.1).\n",
      "libsndfile1 is already the newest version (1.0.28-7ubuntu0.1).\n",
      "sox is already the newest version (14.4.2+git20190427-2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: unidecode in /opt/conda/lib/python3.8/site-packages (1.3.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nemo_toolkit[all]\n",
      "  Cloning https://github.com/NVIDIA/NeMo.git (to revision main) to /tmp/pip-install-3hdq52c5/nemo-toolkit_041a7e8147f4492ea4407389b985771b\n",
      "  Running command git clone -q https://github.com/NVIDIA/NeMo.git /tmp/pip-install-3hdq52c5/nemo-toolkit_041a7e8147f4492ea4407389b985771b\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install wget\n",
    "!apt-get install sox libsndfile1 ffmpeg\n",
    "!pip install unidecode\n",
    "!pip install matplotlib>=3.3.2\n",
    "\n",
    "## Install NeMo\n",
    "BRANCH = 'main'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
    "\n",
    "## Grab the config we'll use in this example\n",
    "!mkdir configs\n",
    "!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/asr/conf/citrinet/config_bpe.yaml\n",
    "\n",
    "\"\"\"\n",
    "Remember to restart the runtime for the kernel to pick up any upgraded packages (e.g. matplotlib)!\n",
    "Alternatively, you can uncomment the exit() below to crash and restart the kernel, in the case\n",
    "that you want to use the \"Run All Cells\" (or similar) option.\n",
    "\"\"\"\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPuyTHGTm8Q-"
   },
   "source": [
    "Let's begin constructing an ASR model that will use the subword tokenizer for its dataset pre-processing and post-processing steps.\n",
    "\n",
    "We will use a Citrinet model to demonstrate the usage of subword tokenization models for training and inference. Citrinet is a [QuartzNet-like architecture](https://arxiv.org/abs/1910.10261), but it uses subword-tokenization along with 8x subsampling and [Squeeze-and-Excitation](https://arxiv.org/abs/1709.01507) to achieve strong accuracy in transcriptions while still using non-autoregressive decoding for efficient inference.\n",
    "\n",
    "We'll be using the **Neural Modules (NeMo) toolkit** for this part, so if you haven't already, you should download and install NeMo and its dependencies. To do so, just follow the directions on the [GitHub page](https://github.com/NVIDIA/NeMo), or in the [documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/).\n",
    "\n",
    "NeMo let us easily hook together the components (modules) of our model, such as the data layer, intermediate layers, and various losses, without worrying too much about implementation details of individual parts or connections between modules. NeMo also comes with complete models which only require your data and hyperparameters for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jALgpGLjmaCw"
   },
   "outputs": [],
   "source": [
    "# NeMo's \"core\" package\n",
    "import nemo\n",
    "print(nemo.__version__)\n",
    "# NeMo's ASR collection - this collections contains complete ASR models and\n",
    "# building blocks (modules) for ASR\n",
    "import nemo.collections.asr as nemo_asr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDTC4fXZ5QnT"
   },
   "source": [
    "## Cross-Language Transfer Learning\n",
    "\n",
    "Transfer learning is an important machine learning technique that uses a modelâ€™s knowledge of one task to perform better on another. Fine-tuning is one of the techniques to perform transfer learning. It is an essential part of the recipe for many state-of-the-art results where a base model is first pretrained on a task with abundant training data and then fine-tuned on different tasks of interest where the training data is less abundant or even scarce.\n",
    "\n",
    "In ASR you might want to do fine-tuning in multiple scenarios, for example, when you want to improve your model's performance on a particular domain (medical, financial, etc.) or accented speech. You can even transfer learn from one language to another! Check out [this paper](https://arxiv.org/abs/2005.04290) for examples.\n",
    "\n",
    "Transfer learning with NeMo is simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IN0LbDbY5YR1"
   },
   "source": [
    "-----\n",
    "First, let's create another tokenizer - perhaps using a larger vocabulary size than the small tokenizer we created earlier. Also we swap out `sentencepiece` for `BERT Word Piece` tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "asr_model = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(model_name=\"stt_en_conformer_ctc_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what kind of vocabulary/alphabet the model has right now\n",
    "print(asr_model.decoder.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(asr_model.decoder.vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BBtk30g5sHJ"
   },
   "source": [
    "Now let's update the vocabulary in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ey9CUkJ5o56"
   },
   "outputs": [],
   "source": [
    "# Lets change the tokenizer vocabulary by passing the path to the new directory,\n",
    "# and also change the type\n",
    "asr_model.change_vocabulary(\n",
    "    new_tokenizer_dir=\"../data_preparation/data/processed/tokenizer/tokenizer_spe_bpe_v1024/\",\n",
    "    new_tokenizer_type=\"bpe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZ3sf2P26SiA"
   },
   "source": [
    "After this, our decoder has completely changed, but our encoder (where most of the weights are) remained intact. Let's fine tune-this model for 20 epochs on AN4 dataset. We will also use the smaller learning rate from ``new_opt` (see the \"After Training\" section)`.\n",
    "\n",
    "**Note**: For this demonstration, we will also freeze the encoder to speed up finetuning (since both tokenizers are built on the same train set), but in general it should not be done for proper training on a new language (or on a different corpus than the original train corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(asr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grab the config we'll use in this example\n",
    "BRANCH='main'\n",
    "!mkdir configs\n",
    "!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/asr/conf/conformer/conformer_ctc_bpe.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "with initialize(config_path=\"./configs/\"):\n",
    "    cfg = compose(config_name=\"conformer_ctc_bpe.yaml\")\n",
    "    print(cfg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['model']['train_ds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = cfg.model.sample_rate\n",
    "print(f\"The sample_rate = {sample_rate}\")\n",
    "\n",
    "ds_sample_rate = cfg.model.train_ds.sample_rate\n",
    "print(f\"The ds_sample_rate = {ds_sample_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = cfg\n",
    "\n",
    "import copy\n",
    "new_opt = copy.deepcopy(params.model.optim)\n",
    "new_opt.lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update paths to dataset\n",
    "params.model.train_ds.manifest_filepath = '../data_preparation/data/processed/train_manifest_merged.json'\n",
    "params.model.train_ds.sample_rate = 16000\n",
    "params.model.train_ds.batch_size = 1\n",
    "\n",
    "params.model.validation_ds.manifest_filepath = ['../data_preparation/data/processed/test_manifest_merged.json', '../data_preparation/data/processed/dev_manifest_merged.json']\n",
    "params.model.validation_ds.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['model']['train_ds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.model.encoder.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7m_CRtH46BjO"
   },
   "outputs": [],
   "source": [
    "# Use the smaller learning rate we set before\n",
    "asr_model.setup_optimization(optim_config=new_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model._cfg.optim.sched.d_model = params.model.encoder.d_model\n",
    "asr_model._cfg.trainer.log_every_n_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.model.optim.sched.d_model = params.model.encoder.d_model\n",
    "print(params.model.optim.sched.d_model)\n",
    "\n",
    "# Point to the data we'll use for fine-tuning as the training set\n",
    "asr_model.setup_training_data(train_data_config=params.model.train_ds)\n",
    "\n",
    "# Point to the new validation data for fine-tuning\n",
    "asr_model.setup_validation_data(val_data_config=params['model']['validation_ds'])\n",
    "\n",
    "# Freeze the encoder layers (should not be done for finetuning, only done for demo)\n",
    "asr_model.encoder.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCmUWZLD63d9"
   },
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "if COLAB_ENV:\n",
    "  %load_ext tensorboard\n",
    "  %tensorboard --logdir lightning_logs/\n",
    "else:\n",
    "  print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s ../data_preparation/data data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fs2aK7xB6pAd"
   },
   "outputs": [],
   "source": [
    "# And now we can create a PyTorch Lightning trainer and call `fit` again.\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "trainer = pl.Trainer(devices=1, accelerator='gpu', max_epochs=20)\n",
    "trainer.fit(asr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a33WIRR9B_gR"
   },
   "source": [
    "So we get fast convergence even though the decoder vocabulary is double the size and we freeze the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alykABQ3CNpf"
   },
   "source": [
    "### Fast Training\n",
    "\n",
    "Last but not least, we could simply speed up training our model! If you have the resources, you can speed up training by splitting the workload across multiple GPUs. Otherwise (or in addition), there's always mixed precision training, which allows you to increase your batch size.\n",
    "\n",
    "You can use [PyTorch Lightning's Trainer object](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html?highlight=Trainer) to handle mixed-precision and distributed training for you. Below are some examples of flags you would pass to the `Trainer` to use these features:\n",
    "\n",
    "```python\n",
    "# Mixed precision:\n",
    "trainer = pl.Trainer(amp_level='O1', precision=16)\n",
    "\n",
    "# Trainer with a distributed backend:\n",
    "trainer = pl.Trainer(devices=2, num_nodes=2, accelerator='gpu', strategy='dp')\n",
    "\n",
    "# Of course, you can combine these flags as well.\n",
    "```\n",
    "\n",
    "Finally, have a look at [example scripts in NeMo repository](https://github.com/NVIDIA/NeMo/blob/stable/examples/asr/asr_ctc/speech_to_text_ctc_bpe.py) which can handle mixed precision and distributed training using command-line arguments."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ASR_with_Subword_Tokenization.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
