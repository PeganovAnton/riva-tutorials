{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c11bba0",
   "metadata": {},
   "source": [
    "# Filter, Mix and Train/Test Split\n",
    "\n",
    "In this notebook we apply a simple filter to filter outlying data points. Then, we combine the three datasets into a single one and do train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6ff42c",
   "metadata": {},
   "source": [
    "# Filter\n",
    "\n",
    "We implement a simple filter here, to filter out samples that:\n",
    " - are too long (>20s), too short (<0.1s) or empty. \n",
    " - contain characters other than those in the German alphabet, punctuation marks and numbers.\n",
    "\n",
    "Advanced filter to weed out out samples that are considered 'noisy', that is, samples having very high WER (word error rate) or CER (character error rate) w.r.t. a previously trained German models are left as an advanced exercice for interested readers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80730973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import json\n",
    "import string\n",
    "\n",
    "def load_jsonl(filepath):\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf_8') as fp:\n",
    "        inlines = fp.readlines()\n",
    "        for line in inlines:\n",
    "            if line.startswith(\"//\") or line.strip() == '':\n",
    "                continue\n",
    "            row = json.loads(line)\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "def dump_jsonl(filepath, data):\n",
    "    with open(filepath, 'w') as fp:\n",
    "        for datum in data:\n",
    "            row = json.dumps(datum, ensure_ascii=False)\n",
    "            fp.write(row)\n",
    "            fp.write('\\n')\n",
    "\n",
    "german_alphabet = set(' abcdefghijklmnopqrstuvwxyzäöüß'+string.punctuation+'0123456789')\n",
    " \n",
    "def filter_manifest(input_manifest, output_manifest, min_duration=0.1, max_duration=20):\n",
    "    utterances = load_jsonl(input_manifest)\n",
    "    filtered_utterances = []\n",
    "    invalid_chars = set()\n",
    "    for i in tqdm.tqdm(range(len(utterances))):\n",
    "        if (utterances[i]['duration'] > max_duration) and (utterances[i]['duration'] < min_duration):\n",
    "            continue\n",
    "        if len(set(utterances[i]['text_original'].lower())-german_alphabet)>0: # text containing non-German characters\n",
    "            invalid_chars = invalid_chars.union(set(utterances[i]['text_original'].lower())-german_alphabet)            \n",
    "            continue\n",
    "        \n",
    "        filtered_utterances.append(utterances[i])\n",
    "    print(\"Number of utterances filtered out: \", len(utterances) - len(filtered_utterances))    \n",
    "    print(\"Invalid characters:\", ''.join(list(invalid_chars)))\n",
    "    \n",
    "    dump_jsonl(output_manifest, filtered_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30e8e8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  ./data/processed/mls/mls_train_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 23497/23497 [00:00<00:00, 163329.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  0\n",
      "Invalid characters: \n",
      "Processing  ./data/processed/mls/mls_dev_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 3469/3469 [00:00<00:00, 159734.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  6\n",
      "Invalid characters: àèé\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  ./data/processed/mls/mls_test_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 3394/3394 [00:00<00:00, 155981.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  2\n",
      "Invalid characters: é\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  ./data/processed/voxpopuli/voxpopuli_train_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 108473/108473 [00:00<00:00, 182750.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  2101\n",
      "Invalid characters: ćľņó─łáőňâęșéˊí—ūń¸̇şżïøýӧ‟‹ţàą„țúī°èšåñãž´źčğçė§ı›śă⁰đê\n",
      "Processing  ./data/processed/voxpopuli/voxpopuli_dev_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 2109/2109 [00:00<00:00, 180268.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  51\n",
      "Invalid characters: ñíãć—čğăá„‚èé\n",
      "Processing  ./data/processed/voxpopuli/voxpopuli_test_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1968/1968 [00:00<00:00, 179225.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  49\n",
      "Invalid characters: ø—ğóàôá„żèš\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  ./data/processed/mcv/mcv_train_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 196403/196403 [00:00<00:00, 248552.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  5095\n",
      "Invalid characters: 道ûņóáảő«șňďęéəшḫན″ếř临—ūиṩîœş‚ēżõаṭħøýе‹à¡ą–„țú尣⟩ô支òсšŏñůž´ṟčė›カśă−ġễч་ê’”ōćµùłквǐâí無臣“āộмńť′…ěþï‟ʻ»фắ孙о‘→īʿрìåźãæ⟨ğçıạ≡‑ðđë\n",
      "Processing  ./data/processed/mcv/mcv_dev_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 15340/15340 [00:00<00:00, 243739.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  386\n",
      "Invalid characters: ćół辶áőș«âęéəíř“āọş‚ë…ěïø‹ʻ»ąứ–„ț‘ʿôåšñæžãźčğ›ıśă乡ðê’đ幺ō\n",
      "Processing  ./data/processed/mcv/mcv_test_manifest_normalized.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 15340/15340 [00:00<00:00, 248284.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances filtered out:  352\n",
      "Invalid characters: ćנעóồłáả«șęňâéíř“āū”ń̇îşå…ěøýʻ»ǔàắ–„țú‘°ʿòשźšãæžא´čğñçבıśð’ëō\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['mls', 'voxpopuli', 'mcv']:\n",
    "    for subset in ['train', 'dev', 'test']:        \n",
    "        input_manifest = os.path.join('./data/processed/', dataset, f\"{dataset}_{subset}_manifest_normalized.json\")\n",
    "        output_manifest = os.path.join('./data/processed/', dataset, f\"{dataset}_{subset}_manifest_normalized_filtered.json\")\n",
    "        print(\"Processing \", input_manifest)\n",
    "        filter_manifest(input_manifest, output_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fed8860",
   "metadata": {},
   "source": [
    "## Mix and Train/Test Split\n",
    "\n",
    "We keep the train/dev/test structure of the original datasets, and simply merge them together. \n",
    "For other application where certain dataset is over- or under-represented, one might want to apply over sampling or undersampling instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "322b36dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  mls train\n",
      "Processing  voxpopuli train\n",
      "Processing  mcv train\n",
      "Processing  mls dev\n",
      "Processing  voxpopuli dev\n",
      "Processing  mcv dev\n",
      "Processing  mls test\n",
      "Processing  voxpopuli test\n",
      "Processing  mcv test\n"
     ]
    }
   ],
   "source": [
    "for subset in ['train', 'dev', 'test']:\n",
    "    merged_manifest = []\n",
    "    for dataset in ['mls', 'voxpopuli', 'mcv']:    \n",
    "        print(\"Processing \", dataset, subset)\n",
    "        merged_manifest.extend(load_jsonl(os.path.join('./data/processed/', dataset, f\"{dataset}_{subset}_manifest_normalized_filtered.json\")))\n",
    "    output_manifest = os.path.join('./data/processed/', f\"{subset}_manifest_merged.json\")\n",
    "    dump_jsonl(output_manifest, merged_manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b38b8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
