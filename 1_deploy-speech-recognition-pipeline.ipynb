{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/riva_asr_asr-python-advanced-finetune-am-citrinet-tao-deployment/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# How to deploy a Riva Speech Recognition Pipeline\n",
    "In this tutorial, you will learn how to deploy Riva speech recognition models - specifically the **Acoustic model (Citrinet)**, **Language model (ngram)**, and **Inverse Text Normalization (WSFT)** pre-trained models downloaded from NVIDIA NGC. \n",
    "\n",
    "This will serve as a primer for customization tutorials in this lab, which may require configuring the Riva speech pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, ensure you have access to [**NVIDIA NGC**](https://ngc.nvidia.com/signin).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Fetch ASR models from NGC\n",
    "### Download CitriNet Acoustic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CitriNet Acoustic Model is located on NGC [here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechtotext_en_us_citrinet/files?version=deployable_v3.0). Let's download it to a local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "# Create a local directory to save models\n",
    "ASR_MODEL_DIR = os.path.join(os.getcwd(), \"asr-models\")\n",
    "!mkdir -p $ASR_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model download-version \"nvidia/tao/speechtotext_en_us_citrinet:deployable_v3.0\" --dest $ASR_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect downloaded files\n",
    "AM_PATH = os.path.join(ASR_MODEL_DIR, \"speechtotext_en_us_citrinet_vdeployable_v3.0\")\n",
    "!ls $AM_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download the n-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n-gram LM is located on NGC [here]( https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechtotext_en_us_lm/files?version=deployable_v1.1). \n",
    "\n",
    "`NOTE:` This may take a couple of minutes to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model download-version \"nvidia/tao/speechtotext_en_us_lm:deployable_v1.1\" --dest $ASR_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect downloaded files\n",
    "LM_PATH = os.path.join(ASR_MODEL_DIR, \"speechtotext_en_us_lm_vdeployable_v1.1\")\n",
    "!ls $LM_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download Inverse Text Normalization (ITN) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ITN model is located on NGC [here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechtotext_en_us_lm/files?version=deployable_v1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model download-version \"nvidia/tao/inverse_normalization_en_us:deployable_v1.0\" --dest $ASR_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect downloaded files\n",
    "ITN_PATH = os.path.join(ASR_MODEL_DIR, \"inverse_normalization_en_us_vdeployable_v1.0\")\n",
    "!ls $ITN_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Riva ServiceMaker\n",
    "Riva ServiceMaker is a set of tools that aggregates all the necessary artifacts (models, files, configurations, and user settings) for Riva deployment to a target environment. It has two main components: `riva-build` and `riva-deploy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Riva-build\n",
    "\n",
    "This step helps build a Riva-ready version of the model. Itâ€™s only output is an intermediate format (called an RMIR) of an end-to-end pipeline for the supported services within Riva. <br>\n",
    "\n",
    "`riva-build` is responsible for the combination of one or more exported models (`.riva` files) into a single file containing an intermediate format called Riva Model Intermediate Representation (`.rmir`). This file contains a deployment-agnostic specification of the whole end-to-end pipeline along with all the assets required for the final deployment and inference. For more information, refer to the [documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-customizing.html#pipeline-configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ServiceMaker Docker\n",
    "RIVA_SM_CONTAINER = \"nvcr.io/nvidia/riva/riva-speech:2.2.1-servicemaker\"\n",
    "\n",
    "# Directory where the Acoustic .riva model is stored $MODEL_LOC/*.riva\n",
    "MODEL_LOC = AM_PATH\n",
    "\n",
    "# Name of the .riva file\n",
    "MODEL_NAME = \"citrinet-1024-Jarvis-asrset-3_0-encrypted.riva\"\n",
    "\n",
    "# Key that model is encrypted with, while exporting with TAO\n",
    "KEY = \"tlt_encode\"\n",
    "\n",
    "# Get the ServiceMaker docker\n",
    "! docker pull $RIVA_SM_CONTAINER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we execute Riva-build to create a pipeline configured for Offline Recognition. This command for reference is also present in the [pipeline configuration](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-customizing.html#pipeline-configuration) section of the docs. <br>\n",
    "Information about the parameters to `riva-build` can be found in the [riva-build optional parameters](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-custom.html?highlight=riva%20build#riva-build-optional-parameters) documentation, or accessible through the `riva-build speech_recognition -h` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We use the Riva servicemaker docker to run riva-build. \n",
    "! docker run --rm --gpus 0 -v $ASR_MODEL_DIR:/data $RIVA_SM_CONTAINER -- \\\n",
    "            riva-build speech_recognition /data/asr.rmir:$KEY /data/speechtotext_en_us_citrinet_vdeployable_v3.0/$MODEL_NAME:$KEY --offline \\\n",
    "            --streaming=False \\\n",
    "            --wfst_tokenizer_model=/data/inverse_normalization_en_us_vdeployable_v1.0/tokenize_and_classify.far \\\n",
    "            --wfst_verbalizer_model=/data/inverse_normalization_en_us_vdeployable_v1.0/verbalize.far \\\n",
    "            --name=citrinet-1024-en-US-asr-streaming \\\n",
    "            --ms_per_timestep=80 \\\n",
    "            --featurizer.use_utterance_norm_params=False \\\n",
    "            --featurizer.precalc_norm_time_steps=0 \\\n",
    "            --featurizer.precalc_norm_params=False \\\n",
    "            --vad.residue_blanks_at_start=-2 \\\n",
    "            --chunk_size=300 \\\n",
    "            --left_padding_size=0. \\\n",
    "            --right_padding_size=0. \\\n",
    "            --decoder_type=flashlight \\\n",
    "            --flashlight_decoder.asr_model_delay=-1 \\\n",
    "            --decoding_language_model_binary=/data/speechtotext_en_us_lm_vdeployable_v1.1/riva_asr_train_datasets_3gram.binary  \\\n",
    "            --decoding_vocab=/data/speechtotext_en_us_lm_vdeployable_v1.1/flashlight_decoder_vocab.txt  \\\n",
    "            --flashlight_decoder.lm_weight=0.2 \\\n",
    "            --flashlight_decoder.word_insertion_score=0.2 \\\n",
    "            --flashlight_decoder.beam_threshold=20. \\\n",
    "            --language_code=en-US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the .rmir\n",
    "!ls -lt $ASR_MODEL_DIR/*.rmir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riva-deploy\n",
    "\n",
    "The deployment tool takes as input one or more Riva Model Intermediate Representation (RMIR) files and a target model repository directory. It creates an ensemble configuration specifying the pipeline for the execution and finally writes all those assets to the output model repository directory.\n",
    "\n",
    "`NOTE`: This step may take about 10 mins to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir:key output-dir-for-repository\n",
    "! docker run --rm --gpus 0 -v $ASR_MODEL_DIR:/data $RIVA_SM_CONTAINER -- \\\n",
    "            riva-deploy -f  /data/asr.rmir:$KEY /data/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the models directory\n",
    "!ls -lt $ASR_MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Start the Riva Server\n",
    "After the model repository is generated, we are ready to start the Riva server. First, download the Riva Quick Start resources from NGC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Riva Quick Start guide\n",
    "The Riva Quick Start guide contains easy-to-use scripts to download and deploy models. \n",
    "\n",
    "`NOTE:` The scripts in Quick Start can download and deploy the default models. We downloaded the ASR models above just to demonstrate how to use Riva ServiceMaker tools, which will be used during customization tutorials to re-deploy the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads the quick start directory to a folder in the current directory and uncompresses it\n",
    "!ngc registry resource download-version \"nvidia/riva/riva_quickstart:2.2.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Riva Quick Start directory\n",
    "RIVA_QSG = os.path.join(os.getcwd(), \"riva_quickstart_v2.2.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configure Riva Quick Start \n",
    "This configures the scripts to deploy the ASR models we obtained as a result of Riva servicemaker tools in the previous section. <br>\n",
    "For this, we modify the `config.sh` file to enable relevant Riva services (ASR for the Citrinet model), provide the encryption key, and path to the model repository (`riva_model_loc`) generated in the previous step among other configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $RIVA_QSG/config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if above the model repository is generated at `$MODEL_LOC/models`, then you can specify `riva_model_loc` as the same directory as `MODEL_LOC`. <br>\n",
    "\n",
    "Pretrained versions of models specified in `models_asr/nlp/tts` are fetched from NGC. Since we are using our custom model, we can comment it in `models_asr` (and any others that are not relevant to your use case). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### config.sh snippet\n",
    "```\n",
    "# Enable or Disable Riva Services \n",
    "service_enabled_asr=true\n",
    "service_enabled_nlp=false                                                      ## MAKE CHANGES HERE - SET TO FALSE\n",
    "service_enabled_tts=false                                                     ## MAKE CHANGES HERE - SET TO FALSE\n",
    "\n",
    "# Specify one or more GPUs to use\n",
    "# specifying more than one GPU is currently an experimental feature, and may result in undefined behaviours.\n",
    "gpus_to_use=\"device=0\"\n",
    "\n",
    "# Specify the encryption key to use to deploy models\n",
    "MODEL_DEPLOY_KEY=\"tlt_encode\"\n",
    "\n",
    "# Locations to use for storing models artifacts\n",
    "#\n",
    "# If an absolute path is specified, the data will be written to that location\n",
    "# Otherwise, a docker volume will be used (default).\n",
    "#\n",
    "# riva_init.sh will create a `rmir` and `models` directory in the volume or\n",
    "# path specified. \n",
    "#\n",
    "# RMIR ($riva_model_loc/rmir)\n",
    "# Riva uses an intermediate representation (RMIR) for models\n",
    "# that are ready to deploy but not yet fully optimized for deployment. Pretrained\n",
    "# versions can be obtained from NGC (by specifying NGC models below) and will be\n",
    "# downloaded to $riva_model_loc/rmir by `riva_init.sh`\n",
    "# \n",
    "# Custom models produced by NeMo or TAO and prepared using riva-build\n",
    "# may also be copied manually to this location $(riva_model_loc/rmir).\n",
    "#\n",
    "# Models ($riva_model_loc/models)\n",
    "# During the riva_init process, the RMIR files in $riva_model_loc/rmir\n",
    "# are inspected and optimized for deployment. The optimized versions are\n",
    "# stored in $riva_model_loc/models. The riva server exclusively uses these\n",
    "# optimized versions.\n",
    "riva_model_loc=\"<add path>\"                              ## MAKE CHANGES HERE (Replace with ASR_MODEL_DIR)                      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure to do the following before moving forward:**\n",
    "1. In the file navigator in Jupyter Lab, navigate to riva_quickstart_v2.* and open config.sh\n",
    "2. Configure settings as shown in the snippet above\n",
    "   - Set nlp and tts services to false\n",
    "   - Configure the riva_model_loc path to where the models resulting from riva-deploy are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set `riva-model-loc` to where the models resulting from riva-deploy are stored. In our case it is ASR_MODEL_DIR\n",
    "!echo $ASR_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have permission to execute these scripts\n",
    "! cd $RIVA_QSG && chmod +x ./riva_start.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Riva Start to start the server. This will deploy your model(s).\n",
    "! cd $RIVA_QSG && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Inference\n",
    "Once the Riva server is up and running with the models, you can send inference requests querying the server. \n",
    "\n",
    "To send gRPC requests, you can install the Riva Python API bindings for the client. This is available as a `pip` `.whl` file with the Quick Start resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the Client API Bindings\n",
    "! cd $RIVA_QSG && pip3 install riva_api-2.2.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Riva Server and Run Automatic Speech Recognition\n",
    "The following cells queries the Riva server (using gRPC) with an input audio to yield a transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import IPython.display as ipd\n",
    "import grpc\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import riva_api.riva_audio_pb2 as ra # RIVA 2.0.0 and above\n",
    "except:\n",
    "    import riva_api.audio_pb2 as ra\n",
    "import riva_api.riva_asr_pb2 as rasr\n",
    "import riva_api.riva_asr_pb2_grpc as rasr_srv\n",
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample audio file from local disk\n",
    "# This example uses a .wav file with LINEAR_PCM encoding.\n",
    "audio_file = \"audio_samples/en-US_wordboosting_sample1.wav\"\n",
    "with io.open(audio_file, 'rb') as fh:\n",
    "    content = fh.read()\n",
    "    \n",
    "# Listen to the sample audio we are looking to transcribe\n",
    "ipd.Audio(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"localhost:50051\"\n",
    "\n",
    "wf = wave.open(audio_file, 'rb')\n",
    "with open(audio_file, 'rb') as fh:\n",
    "    data = fh.read()\n",
    "\n",
    "channel = grpc.insecure_channel(server)\n",
    "client = rasr_srv.RivaSpeechRecognitionStub(channel)\n",
    "config = rasr.RecognitionConfig(\n",
    "    encoding=ra.AudioEncoding.LINEAR_PCM,\n",
    "    sample_rate_hertz=wf.getframerate(),\n",
    "    language_code=\"en-US\",\n",
    "    max_alternatives=1,\n",
    "    enable_automatic_punctuation=False,\n",
    "    audio_channel_count=1\n",
    ")\n",
    "\n",
    "request = rasr.RecognizeRequest(config=config, audio=data)\n",
    "\n",
    "response = client.Recognize(request)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you are all set with a speech recognition pipeline!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
